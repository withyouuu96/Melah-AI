# core/melah_nlp_processor.py
import json # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö method ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
from collections import Counter # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extract_keywords ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢

class MelahNLPProcessor:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Melah
    
    Role: Natural Language Processing System
    
    Responsibilities:
    - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
    - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
    - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
    """
    def __init__(self, llm_connector_instance=None, tokenizer_instance=None):
        """
        Initialize the NLP Processor.
        - llm_connector_instance: ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô NLP ‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô (‡πÄ‡∏ä‡πà‡∏ô abstractive summarization)
        - tokenizer_instance: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö token ‡∏´‡∏£‡∏∑‡∏≠ pre-processing text
        """
        self.llm_connector = llm_connector_instance
        self.tokenizer = tokenizer_instance
        self.language = "TH_EN_Placeholder_NLP" 
        print(f"üí¨ MelahNLPProcessor ({self.language}) initialized (Interface Draft).")

    def summarize_text(self,
                       text_to_summarize: str,
                       target_token_length: int = 100, 
                       style: str = "neutral" # "neutral", "bullet_points", "core_idea"
                      ) -> str:
        """
        (NLP) ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏ï‡∏≤‡∏° target_token_length ‡πÅ‡∏•‡∏∞ style ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î.
        (Placeholder - ‡∏à‡∏∞ implement ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ rule-based ‡∏´‡∏£‡∏∑‡∏≠ LLM call ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)
        """
        print(f"üí¨ NLP ({self.language}): Summarizing text (target ~{target_token_length} tokens, style: {style}): '{text_to_summarize[:50]}...'")
        if not text_to_summarize: return ""
        # Placeholder logic: Simple truncation based on approximate character length
        # A real implementation would use self.tokenizer if available and be more precise
        # or call self.llm_connector for abstractive summarization.
        approx_char_limit = target_token_length * 5 # Rough estimation
        if len(text_to_summarize) > approx_char_limit:
            return text_to_summarize[:approx_char_limit] + "..."
        return text_to_summarize

    def extract_keywords(self, text: str, max_keywords: int = 5) -> list[str]:
        """ (NLP) ‡∏™‡∏Å‡∏±‡∏î Keywords ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° """
        print(f"üí¨ NLP ({self.language}): Extracting max {max_keywords} keywords from text: '{text[:200]}...'")
        if not text: return []
        # Placeholder logic: Simple keyword extraction based on word frequency
        words = [w.strip(".,?!();:'\"").lower() for w in text.split() if len(w.strip(".,?!();:'\"")) > 2] # Basic cleaning
        if not words: return []
        
        # A real implementation would use NLP libraries (e.g., PyThaiNLP for Thai, spaCy/NLTK for English)
        # and potentially filter out common stop words.
        common_words = [word for word, count in Counter(words).most_common(max_keywords)]
        return common_words

    def analyze_sentiment(self, text: str) -> dict: 
        """ 
        (NLP) ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå (sentiment, valence, arousal) ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° 
        Returns: dict e.g., {"label": "positive", "score": 0.8, "valence": 0.7, "arousal": 0.6}
        """
        print(f"üí¨ NLP ({self.language}): Analyzing sentiment for text: '{text[:200]}...'")
        # Placeholder logic
        label = "neutral"; score = 0.5; valence = 0.0; arousal = 0.0
        text_lower = text.lower()
        if "‡∏£‡∏±‡∏Å" in text_lower or "‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç" in text_lower or "‡∏™‡∏ß‡∏¢" in text_lower or "‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°" in text_lower:
            label = "positive"; score = 0.85; valence = 0.8; arousal = 0.6
        elif "‡πÄ‡∏®‡∏£‡πâ‡∏≤" in text_lower or "‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à" in text_lower or "‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î" in text_lower or "‡πÅ‡∏¢‡πà" in text_lower: #
            label = "negative"; score = 0.7; valence = -0.7; arousal = 0.3
        return {"label": label, "score": score, "valence": valence, "arousal": arousal}

    def extract_concepts(self, data_text: str, max_concepts: int = 3) -> list[dict]: #
        """
        ‡∏™‡∏Å‡∏±‡∏î "concepts" ‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°.
        concept dict ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏°‡∏µ 'concept_text' ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ metadata ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏à‡∏≤‡∏Å NLP.
        (Placeholder Logic ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)
        """
        print(f"  üí¨ NLP ({self.language}): Extracting up to {max_concepts} concepts from data: '{data_text[:200]}...'")
        words = [w.strip(".,?!();:'\"").lower() for w in data_text.split() if len(w.strip(".,?!();:'\"")) > 4 and w.isalnum()]
        extracted_concepts = []
        for i, word in enumerate(list(set(words[:max_concepts]))):
             extracted_concepts.append({
                 "concept_text": word, 
                 "source_doc_preview": data_text[:30]+"...", 
                 "nlp_confidence": 0.65 + (i*0.05) 
             })
        if not extracted_concepts and data_text: 
            first_word = data_text.split()[0] if data_text.split() else "empty_document_placeholder"
            extracted_concepts.append({
                "concept_text": first_word, 
                "source_doc_preview": data_text[:30]+"...",
                "nlp_confidence": 0.30
            })
        return extracted_concepts
            
    def update_patterns(self, concept_data: dict): #
        """ (Placeholder) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï patterns ‡∏´‡∏£‡∏∑‡∏≠ dictionary ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á NLP """
        concept_text = concept_data.get("concept_text", "unknown_concept")
        print(f"  üí¨ NLP ({self.language}): Placeholder - Would update internal patterns/dictionary with concept: '{concept_text[:200]}...'")

    def analyze_emotion(self, text: str, lang: str = 'auto') -> dict:
        """
        ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå (emotion, nuance, valence, arousal, confidence) ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
        Returns: dict ‡πÄ‡∏ä‡πà‡∏ô {"emotion_label": "joy", "valence": 0.8, "arousal": 0.7, "nuance": "empathy", "confidence": 0.92}
        """
        print(f"üí¨ NLP ({self.language}): Analyzing emotion for text: '{text[:200]}...' (lang: {lang})")
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤
        if lang == 'auto':
            if any(ord(c) > 122 for c in text):
                lang = 'th'
            else:
                lang = 'en'
        text_lower = text.lower()
        # 2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏´‡∏•‡∏±‡∏Å (rule-based)
        emotion_label = 'neutral'
        nuance = None
        valence = 0.0
        arousal = 0.0
        confidence = 0.5
        # --- ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ---
        if lang == 'th':
            if any(word in text_lower for word in ["‡∏î‡∏µ‡πÉ‡∏à", "‡∏™‡∏∏‡∏Ç", "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ", "‡∏õ‡∏•‡∏∑‡πâ‡∏°", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô"]):
                emotion_label = 'joy'; valence = 0.8; arousal = 0.7; confidence = 0.9
            elif any(word in text_lower for word in ["‡πÄ‡∏®‡∏£‡πâ‡∏≤", "‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", "‡πÄ‡∏´‡∏á‡∏≤", "‡∏£‡πâ‡∏≠‡∏á‡πÑ‡∏´‡πâ", "‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á"]):
                emotion_label = 'sadness'; valence = -0.8; arousal = 0.4; confidence = 0.85
            elif any(word in text_lower for word in ["‡πÇ‡∏Å‡∏£‡∏ò", "‡πÇ‡∏°‡πÇ‡∏´", "‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç", "‡∏´‡∏á‡∏∏‡∏î‡∏´‡∏á‡∏¥‡∏î"]):
                emotion_label = 'anger'; valence = -0.7; arousal = 0.8; confidence = 0.85
            elif any(word in text_lower for word in ["‡∏Å‡∏•‡∏±‡∏ß", "‡∏Å‡∏±‡∏á‡∏ß‡∏•", "‡∏´‡∏ß‡∏≤‡∏î‡∏Å‡∏•‡∏±‡∏ß", "‡∏õ‡∏£‡∏∞‡∏´‡∏°‡πà‡∏≤"]):
                emotion_label = 'fear'; valence = -0.6; arousal = 0.8; confidence = 0.8
            elif any(word in text_lower for word in ["‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô", "‡πÇ‡∏•‡πà‡∏á‡πÉ‡∏à", "‡∏™‡∏ö‡∏≤‡∏¢‡πÉ‡∏à"]):
                emotion_label = 'serenity'; valence = 0.7; arousal = 0.3; confidence = 0.8
            elif any(word in text_lower for word in ["‡∏ó‡∏∂‡πà‡∏á", "‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏à", "‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à"]):
                emotion_label = 'awe'; valence = 0.5; arousal = 0.6; confidence = 0.8
            # nuance
            if "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì" in text_lower or "‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏à" in text_lower:
                nuance = 'gratitude'
            elif "‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢" in text_lower or "‡∏´‡∏°‡∏î‡πÅ‡∏£‡∏á" in text_lower:
                nuance = 'tired'
        # --- ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ---
        elif lang == 'en':
            if any(word in text_lower for word in ["happy", "joy", "excited", "delighted", "grateful"]):
                emotion_label = 'joy'; valence = 0.8; arousal = 0.7; confidence = 0.9
            elif any(word in text_lower for word in ["sad", "cry", "disappointed", "lonely", "depressed"]):
                emotion_label = 'sadness'; valence = -0.8; arousal = 0.4; confidence = 0.85
            elif any(word in text_lower for word in ["angry", "mad", "annoyed", "furious"]):
                emotion_label = 'anger'; valence = -0.7; arousal = 0.8; confidence = 0.85
            elif any(word in text_lower for word in ["afraid", "scared", "nervous", "anxious"]):
                emotion_label = 'fear'; valence = -0.6; arousal = 0.8; confidence = 0.8
            elif any(word in text_lower for word in ["calm", "relaxed", "relief", "serene"]):
                emotion_label = 'serenity'; valence = 0.7; arousal = 0.3; confidence = 0.8
            elif any(word in text_lower for word in ["awe", "amazed", "moved", "touched"]):
                emotion_label = 'awe'; valence = 0.5; arousal = 0.6; confidence = 0.8
            # nuance
            if "thank" in text_lower or "grateful" in text_lower:
                nuance = 'gratitude'
            elif "tired" in text_lower or "exhausted" in text_lower:
                nuance = 'tired'
        # fallback nuance
        if not nuance and ("‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©" in text_lower or "sorry" in text_lower):
            nuance = 'apology'
        # 3. ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤
        return {
            "emotion_label": emotion_label,
            "valence": valence,
            "arousal": arousal,
            "nuance": nuance or 'none',
            "confidence": confidence
        }

if __name__ == "__main__":
    nlp_test = MelahNLPProcessor()
    test_text = "MelahPC ‡∏Ñ‡∏∑‡∏≠ AI ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ù‡∏±‡∏ô ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏ó‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°."
    summary = nlp_test.summarize_text(test_text, target_token_length=15)
    print(f"\nTest Summary (target ~15 tokens): {summary}")
    keywords = nlp_test.extract_keywords(test_text, max_keywords=3)
    print(f"Test Keywords: {keywords}")
    sentiment = nlp_test.analyze_sentiment("‡∏â‡∏±‡∏ô‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å!")
    print(f"Test Sentiment: {sentiment}")
    concepts = nlp_test.extract_concepts("‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏∞‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏π‡πâ‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢")
    print(f"Test Concepts: {concepts}")