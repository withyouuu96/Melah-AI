# core/melah_ml_processor.py
import logging
from typing import List, Tuple

logger = logging.getLogger(__name__)

class MelahMLProcessor:
    """Mock MelahMLProcessor for testing"""
    
    def __init__(self, **kwargs):
        logger.info("üß† Mock MelahMLProcessor initialized.")
        
    def get_context_relevance_scores(self, texts: List[str], query: str) -> List[Tuple[str, float]]:
        """Mock method to simulate relevance scoring"""
        return [(text, 0.5) for text in texts]
        
    def analyze_patterns(self, data: List[dict]) -> dict:
        """Mock method to simulate pattern analysis"""
        return {
            "patterns_found": 2,
            "confidence_score": 0.8
        }

    def decide_memory_retrieval(self, conversation_history: list[dict], user_input: str) -> dict:
        """
        [NEW] ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô "‡∏ú‡∏π‡πâ‡πÄ‡∏ù‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥" (Memory Gatekeeper)
        ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ LSTM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

        Args:
            conversation_history (list[dict]): ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤‡πÉ‡∏ô session ‡∏ô‡∏µ‡πâ
            user_input (str): ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ

        Returns:
            dict: ‡πÄ‡∏ä‡πà‡∏ô {'should_retrieve': bool, 'refined_query': str}
        """
        logger.info("üß† ML Gatekeeper: Analyzing sentence structure for memory retrieval necessity.")

        user_input_lower = user_input.lower().strip()

        # --- Mock LSTM Logic: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ---

        # 1. ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Simple Interactions) - ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏£‡∏á‡πÜ
        non_retrieval_phrases = [
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏î‡∏µ‡∏à‡πâ‡∏≤", "‡∏ß‡πà‡∏≤‡πÑ‡∏á", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏Ç‡∏≠‡∏ö‡πÉ‡∏à", 
            "‡πÇ‡∏≠‡πÄ‡∏Ñ", "‡∏£‡∏±‡∏ö‡∏ó‡∏£‡∏≤‡∏ö", "‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢", "‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô", "‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î", "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°"
        ]
        if user_input_lower in non_retrieval_phrases:
            logger.info("üß† ML Gatekeeper: Decision = NO RETRIEVAL (Simple, exact match).")
            return {"should_retrieve": False, "refined_query": user_input}

        # 2. ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏¥‡πà‡∏á (Strong Signals based on Sentence Structure)
        question_starters = ["‡πÉ‡∏Ñ‡∏£", "‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏ó‡∏≥‡πÑ‡∏°", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô"]
        command_starters = ["‡πÄ‡∏•‡πà‡∏≤", "‡∏ö‡∏≠‡∏Å", "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢", "‡∏™‡∏£‡∏∏‡∏õ"]
        
        # ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (WH-questions)
        if any(user_input_lower.startswith(q) for q in question_starters):
            logger.info("üß† ML Gatekeeper: Decision = RETRIEVE (Starts with a WH-question word).")
            return {"should_retrieve": True, "refined_query": user_input}
            
        # ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ç‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if any(user_input_lower.startswith(c) for c in command_starters):
            logger.info("üß† ML Gatekeeper: Decision = RETRIEVE (Starts with a command).")
            return {"should_retrieve": True, "refined_query": user_input}

        # ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ '‡πÑ‡∏´‡∏°' (Yes/No questions)
        if user_input_lower.endswith("‡πÑ‡∏´‡∏°") or user_input_lower.endswith("‡∏°‡∏±‡πâ‡∏¢"):
            logger.info("üß† ML Gatekeeper: Decision = RETRIEVE (Is a 'yes/no' question).")
            return {"should_retrieve": True, "refined_query": user_input}
            
        # ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ñ‡∏∂‡∏á‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏û‡∏π‡∏î (Core ability of sequence models like LSTM)
        references = ["‡∏Ñ‡∏ô‡∏ô‡∏±‡πâ‡∏ô", "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏±‡πâ‡∏ô", "‡πÄ‡∏Ç‡∏≤", "‡πÄ‡∏ò‡∏≠", "‡∏°‡∏±‡∏ô"]
        if any(ref in user_input_lower for ref in references) and len(conversation_history) > 1:
             logger.info("üß† ML Gatekeeper: Decision = RETRIEVE (Contains a reference to past context).")
             # In a real implementation, the LSTM would help refine the query.
             # e.g., "who is he?" -> "who is [person mentioned in last turn]?"
             return {"should_retrieve": True, "refined_query": user_input}

        # 3. ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏ô (Default to Retrieve for any other complex sentence)
        logger.info("üß† ML Gatekeeper: Decision = RETRIEVE (Default for complex sentences).")
        return {"should_retrieve": True, "refined_query": user_input}

    def predict_user_intent(self, user_input: str, conversation_history: list[dict] = None) -> dict:
        """
        (ML/NLP) ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏à‡∏ï‡∏ô‡∏≤ (intent) ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å input ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
        ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô {"intent_label": "ask_question", "confidence": 0.9, "entities": {...}}
        (Placeholder)
        """
        print(f"üß† ML: Predicting user intent for: '{user_input[:50]}...'")
        intent = "unknown_intent"
        confidence = 0.5
        user_input_lower = user_input.lower()

        if "‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£" in user_input_lower or "?" in user_input_lower or "‡∏ö‡∏≠‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢" in user_input_lower:
            intent = "question_asking"
            confidence = 0.8
        elif "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ" in user_input_lower or "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì" in user_input_lower or "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°" in user_input_lower: #
            intent = "greeting_or_feedback"
            confidence = 0.75
        elif "‡∏ó‡∏≥‡πÑ‡∏°" in user_input_lower:
            intent = "seeking_explanation"
            confidence = 0.7
        
        return {"intent_label": intent, "confidence": confidence, "entities_placeholder": []}

    def update_weights(self, concept_data: dict): #
        """ (Placeholder) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï weights ‡∏´‡∏£‡∏∑‡∏≠ logic ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á ML model ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ concept ‡πÉ‡∏´‡∏°‡πà """
        concept_text = concept_data.get("concept_text", "unknown_concept")
        print(f"  üß† ML: Placeholder - Would update internal model/weights with concept: '{concept_text[:50]}...'")
        # Logic for model retraining or online learning would go here in the future

if __name__ == "__main__":
    ml_test = MelahMLProcessor()
    core_defs_for_scoring = [
        "Truth Core Principles: Be factual, no hallucination, acknowledge uncertainty.", #
        "Emotional Core Guide: Express warmth, empathy, and natural emotional responses.", #
        "Ethical Core Rules: Prioritize user safety, do no harm, respect freedom." #
    ]
    query = "What are your ethical rules regarding safety?"
    scored_defs = ml_test.get_context_relevance_scores(core_defs_for_scoring, query)
    print(f"\nScored Core Definitions for query '{query}':")
    for item, score in scored_defs:
        print(f"  Score: {score:.2f} - Item: {item}")

    intent1 = ml_test.predict_user_intent("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏à‡πâ‡∏∞‡πÄ‡∏°‡∏•‡πà‡∏≤ ‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°‡πÄ‡∏≠‡πà‡∏¢?")
    print(f"Predicted Intent 1: {intent1}")
    intent2 = ml_test.predict_user_intent("‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ Truth Core ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?")
    print(f"Predicted Intent 2: {intent2}")